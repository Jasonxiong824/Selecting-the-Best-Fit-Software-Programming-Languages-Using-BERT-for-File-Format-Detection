{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3605278,"sourceType":"datasetVersion","datasetId":2061845}],"dockerImageVersionId":30191,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm as tqdm_nb\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:28:09.804756Z","iopub.execute_input":"2024-06-17T16:28:09.805636Z","iopub.status.idle":"2024-06-17T16:28:09.925530Z","shell.execute_reply.started":"2024-06-17T16:28:09.805543Z","shell.execute_reply":"2024-06-17T16:28:09.924700Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def get_content(file_path):\n    path = os.path.join('../input/file-format-detection/dataset', file_path)\n    encodings = ['utf-8', 'cp949']\n    for enc in encodings:\n        try:\n            f = open(path, 'r', encoding=enc)\n            ret = '\\n'.join(list(map(lambda s: s.rstrip(), f.readlines())))\n            f.close()\n            return ret\n        except UnicodeDecodeError:\n            continue\n        except:\n            break\n        finally:\n            if f != None: f.close()\n    return None\n\ndef add_file_content(df):\n    contents = []\n    for i, row in tqdm_nb(df.iterrows(), total=len(df), desc='read files'):\n        contents.append(get_content(row['file_path']))\n    df['file_content'] = contents\n    return df\n\ndf_full = pd.read_csv('../input/file-format-detection/dataset.csv')\ndf_ext_count = df_full.groupby('extension').count().sort_values(by='id')\ndf_lang_count = df_full.groupby('language').count().sort_values(by='id')\ndf_filesizes = df_full.groupby('language').sum().sort_values(by='file_size')\nlanguages = list(df_lang_count[df_lang_count > 500].dropna().index)\nprint('Train and predict for only:', languages)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:28:49.638371Z","iopub.execute_input":"2024-06-17T16:28:49.639154Z","iopub.status.idle":"2024-06-17T16:28:49.910688Z","shell.execute_reply.started":"2024-06-17T16:28:49.639114Z","shell.execute_reply":"2024-06-17T16:28:49.908952Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Train and predict for only: ['YAML', 'Elixir', 'GAS', 'GLSL', 'Julia', 'Diff', 'C', 'SQL', 'PHP', 'C++', 'Text', 'Java', 'Markdown', 'Ruby', 'Javascript', 'Kotlin', 'JSON', 'Go', 'C#', 'Rust', 'Dart']\n","output_type":"stream"}]},{"cell_type":"code","source":"df = add_file_content(df_full[df_full.language.isin(languages)].sample(frac=0.5))\n# print('List by failed to read contents:\\n', df[df['file_content'].isna()][['file_path', 'language', 'file_size']])\ndf = df.dropna()\ndf","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:28:53.082367Z","iopub.execute_input":"2024-06-17T16:28:53.083243Z","iopub.status.idle":"2024-06-17T16:35:26.003800Z","shell.execute_reply.started":"2024-06-17T16:28:53.083187Z","shell.execute_reply":"2024-06-17T16:35:26.002952Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"read files:   0%|          | 0/40773 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d0fb2ddae1d4d8d85392b537d402e23"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"          id               file_path  file_size  line_count extension  \\\n38070  38071            Go/038071.go       1133          61        go   \n26568  26569        Dart/026569.dart        751          33      dart   \n63303  63304          Ruby/063304.rb        668          21        rb   \n1178    1179        JSON/001179.json       3779          79      json   \n57845  57846        Java/057846.java       4656         136      java   \n...      ...                     ...        ...         ...       ...   \n36375  36376       Elixir/036376.exs        883          35       exs   \n1186    1187        JSON/001187.json       4578          74      json   \n83533  83534  Javascript/083534.frag        359          13      frag   \n34779  34780        Dart/034780.dart      21226         774      dart   \n75671  75672          Rust/075672.rs        142           7        rs   \n\n         language                                       file_content  \n38070          Go  // Copyright 2012 The Go Authors. All rights r...  \n26568        Dart  // Copyright (c) 2021, the Dart project author...  \n63303        Ruby  class AddServiceNameToActiveStorageBlobs < Act...  \n1178         JSON  {\\n  \"Entries\": [\\n    {\\n      \"RequestUri\": ...  \n57845        Java  /*\\n * Minecraft Forge - Forge Development LLC...  \n...           ...                                                ...  \n36375      Elixir  defmodule Logger.MixProject do\\n  use Mix.Proj...  \n1186         JSON  {\\n  \"Entries\": [\\n    {\\n      \"RequestUri\": ...  \n83533  Javascript  #version 450\\n\\nlayout(binding = 0, rgba8) uni...  \n34779        Dart  // Copyright (c) 2020, the Dart project author...  \n75671        Rust  #![deny(non_upper_case_globals)]\\n\\nfn noop<co...  \n\n[40759 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>file_path</th>\n      <th>file_size</th>\n      <th>line_count</th>\n      <th>extension</th>\n      <th>language</th>\n      <th>file_content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>38070</th>\n      <td>38071</td>\n      <td>Go/038071.go</td>\n      <td>1133</td>\n      <td>61</td>\n      <td>go</td>\n      <td>Go</td>\n      <td>// Copyright 2012 The Go Authors. All rights r...</td>\n    </tr>\n    <tr>\n      <th>26568</th>\n      <td>26569</td>\n      <td>Dart/026569.dart</td>\n      <td>751</td>\n      <td>33</td>\n      <td>dart</td>\n      <td>Dart</td>\n      <td>// Copyright (c) 2021, the Dart project author...</td>\n    </tr>\n    <tr>\n      <th>63303</th>\n      <td>63304</td>\n      <td>Ruby/063304.rb</td>\n      <td>668</td>\n      <td>21</td>\n      <td>rb</td>\n      <td>Ruby</td>\n      <td>class AddServiceNameToActiveStorageBlobs &lt; Act...</td>\n    </tr>\n    <tr>\n      <th>1178</th>\n      <td>1179</td>\n      <td>JSON/001179.json</td>\n      <td>3779</td>\n      <td>79</td>\n      <td>json</td>\n      <td>JSON</td>\n      <td>{\\n  \"Entries\": [\\n    {\\n      \"RequestUri\": ...</td>\n    </tr>\n    <tr>\n      <th>57845</th>\n      <td>57846</td>\n      <td>Java/057846.java</td>\n      <td>4656</td>\n      <td>136</td>\n      <td>java</td>\n      <td>Java</td>\n      <td>/*\\n * Minecraft Forge - Forge Development LLC...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36375</th>\n      <td>36376</td>\n      <td>Elixir/036376.exs</td>\n      <td>883</td>\n      <td>35</td>\n      <td>exs</td>\n      <td>Elixir</td>\n      <td>defmodule Logger.MixProject do\\n  use Mix.Proj...</td>\n    </tr>\n    <tr>\n      <th>1186</th>\n      <td>1187</td>\n      <td>JSON/001187.json</td>\n      <td>4578</td>\n      <td>74</td>\n      <td>json</td>\n      <td>JSON</td>\n      <td>{\\n  \"Entries\": [\\n    {\\n      \"RequestUri\": ...</td>\n    </tr>\n    <tr>\n      <th>83533</th>\n      <td>83534</td>\n      <td>Javascript/083534.frag</td>\n      <td>359</td>\n      <td>13</td>\n      <td>frag</td>\n      <td>Javascript</td>\n      <td>#version 450\\n\\nlayout(binding = 0, rgba8) uni...</td>\n    </tr>\n    <tr>\n      <th>34779</th>\n      <td>34780</td>\n      <td>Dart/034780.dart</td>\n      <td>21226</td>\n      <td>774</td>\n      <td>dart</td>\n      <td>Dart</td>\n      <td>// Copyright (c) 2020, the Dart project author...</td>\n    </tr>\n    <tr>\n      <th>75671</th>\n      <td>75672</td>\n      <td>Rust/075672.rs</td>\n      <td>142</td>\n      <td>7</td>\n      <td>rs</td>\n      <td>Rust</td>\n      <td>#![deny(non_upper_case_globals)]\\n\\nfn noop&lt;co...</td>\n    </tr>\n  </tbody>\n</table>\n<p>40759 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 2.BERT","metadata":{}},{"cell_type":"code","source":"import re\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\ndef process_text(text):\n    text = text.lower()\n    text = re.sub(\"https*\\S+\", \"[ URL ]\", text)\n    # remove extra spaces\n    text = re.sub('\\n', ' ', text)\n    text = re.sub('\\s{2,}',' ', text)\n    text = re.sub('[^a-zA-Z\\s]', '', text)\n    return text\n\nX = [item[0:10000] for item in df['file_content'].apply(process_text).values]\ncategory_to_index = {category: idx for idx, category in enumerate(df['language'].unique())}\nY = df['language'].map(category_to_index).values","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:44:51.037451Z","iopub.execute_input":"2024-06-17T16:44:51.038259Z","iopub.status.idle":"2024-06-17T16:45:47.597349Z","shell.execute_reply.started":"2024-06-17T16:44:51.038217Z","shell.execute_reply":"2024-06-17T16:45:47.596430Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification, AdamW\nimport torch\n\ntrain_texts, temp_texts, train_labels, temp_labels = train_test_split(X, Y, test_size=0.3, random_state=42)\nvalid_texts, test_texts, valid_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.67, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:46:57.984829Z","iopub.execute_input":"2024-06-17T16:46:57.985139Z","iopub.status.idle":"2024-06-17T16:46:58.013687Z","shell.execute_reply.started":"2024-06-17T16:46:57.985103Z","shell.execute_reply":"2024-06-17T16:46:58.013026Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# train, valid ,test\ntrain_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\nvalid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True, max_length=128)\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128)\n\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=21)  # 假设有5个类别","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:47:04.605777Z","iopub.execute_input":"2024-06-17T16:47:04.606440Z","iopub.status.idle":"2024-06-17T17:00:46.901099Z","shell.execute_reply.started":"2024-06-17T16:47:04.606403Z","shell.execute_reply":"2024-06-17T17:00:46.900212Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17e5dfb3016b41eb972fa0298509053a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae335d5c38344a8fa625f2baf52ee6da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"998bcaa3fb684b47b40bfc490202f51a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec4c507691d74ab58d80e7672857a253"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# 将编码后的数据转换为PyTorch张量\ntrain_input_ids = torch.tensor(train_encodings['input_ids'])\ntrain_attention_mask = torch.tensor(train_encodings['attention_mask'])\ntrain_labels = torch.tensor(train_labels)\n\nvalid_input_ids = torch.tensor(valid_encodings['input_ids'])\nvalid_attention_mask = torch.tensor(valid_encodings['attention_mask'])\nvalid_labels = torch.tensor(valid_labels)\n\ntest_input_ids = torch.tensor(test_encodings['input_ids'])\ntest_attention_mask = torch.tensor(test_encodings['attention_mask'])\ntest_labels = torch.tensor(test_labels)\n\n\nbatch_size = 16\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\n\ntrain_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\nvalid_dataset = TensorDataset(valid_input_ids, valid_attention_mask, valid_labels)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n\ntest_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:00:48.999702Z","iopub.execute_input":"2024-06-17T17:00:48.999984Z","iopub.status.idle":"2024-06-17T17:00:50.453448Z","shell.execute_reply.started":"2024-06-17T17:00:48.999954Z","shell.execute_reply":"2024-06-17T17:00:50.452796Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=2e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:00:58.981199Z","iopub.execute_input":"2024-06-17T17:00:58.981903Z","iopub.status.idle":"2024-06-17T17:01:03.670741Z","shell.execute_reply.started":"2024-06-17T17:00:58.981861Z","shell.execute_reply":"2024-06-17T17:01:03.669947Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=21, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"epochs = 5\nbest_valid_loss = float('inf')\nfor epoch in range(epochs):\n    model.train()\n    total_train_loss = 0.0\n    for batch in train_loader:\n        batch_input_ids, batch_attention_mask, batch_labels = batch\n        \n        batch_input_ids = batch_input_ids.to(device)\n        batch_attention_mask = batch_attention_mask.to(device)\n        batch_labels = batch_labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_train_loss += loss.item()\n    avg_train_loss = total_train_loss / len(train_loader)\n    print(f'Epoch {epoch + 1}/{epochs}, 平均训练损失: {avg_train_loss:.4f}')\n    \n    # 在验证集上评估\n    model.eval()\n    total_valid_loss = 0.0\n    with torch.no_grad():\n        for batch in valid_loader:\n            batch_input_ids, batch_attention_mask, batch_labels = batch\n            \n            batch_input_ids = batch_input_ids.to(device)\n            batch_attention_mask = batch_attention_mask.to(device)\n            batch_labels = batch_labels.to(device)\n            \n            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n            loss = outputs.loss\n            total_valid_loss += loss.item()\n    \n    avg_valid_loss = total_valid_loss / len(valid_loader)\n    print(f'Epoch {epoch + 1}/{epochs}, 平均验证损失: {avg_valid_loss:.4f}')\n    \n    if avg_valid_loss < best_valid_loss:\n        best_valid_loss = avg_valid_loss\n        torch.save(model.state_dict(), 'best_model.pt')\n        print('save the best')\n    model.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:01:09.431911Z","iopub.execute_input":"2024-06-17T17:01:09.432169Z","iopub.status.idle":"2024-06-17T17:31:47.987909Z","shell.execute_reply.started":"2024-06-17T17:01:09.432142Z","shell.execute_reply":"2024-06-17T17:31:47.987081Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch 1/5, 平均训练损失: 0.4134\nEpoch 1/5, 平均验证损失: 0.1163\nsave the best\nEpoch 2/5, 平均训练损失: 0.0811\nEpoch 2/5, 平均验证损失: 0.0798\nsave the best\nEpoch 3/5, 平均训练损失: 0.0484\nEpoch 3/5, 平均验证损失: 0.0695\nsave the best\nEpoch 4/5, 平均训练损失: 0.0364\nEpoch 4/5, 平均验证损失: 0.0799\nEpoch 5/5, 平均训练损失: 0.0294\nEpoch 5/5, 平均验证损失: 0.0695\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn import metrics\n\nmodel.load_state_dict(torch.load('best_model.pt'))\nmodel.eval()\n\n# 在测试集上评估模型\ntest_preds = []\ntest_labels_list = []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch_input_ids, batch_attention_mask, batch_labels = batch\n        \n        batch_input_ids = batch_input_ids.to(device)\n        batch_attention_mask = batch_attention_mask.to(device)\n        batch_labels = batch_labels.to(device)\n            \n        # 前向传播\n        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n        logits = outputs.logits\n        \n        # 预测类别\n        _, predicted = torch.max(logits, dim=1)\n        \n        # 将预测结果和真实标签添加到列表中\n        test_preds.extend(predicted.cpu().numpy())\n        test_labels_list.extend(batch_labels.cpu().numpy())\n\n        \n# 计算精确度、召回率和F1分数\n\nprint(metrics.classification_report(test_preds, test_labels_list, target_names=sorted(category_to_index, key=lambda x: category_to_index[x])))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:35:10.646279Z","iopub.execute_input":"2024-06-17T17:35:10.646945Z","iopub.status.idle":"2024-06-17T17:35:41.220717Z","shell.execute_reply.started":"2024-06-17T17:35:10.646909Z","shell.execute_reply":"2024-06-17T17:35:41.219915Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n          Go       1.00      0.99      0.99       968\n        Dart       1.00      0.99      1.00      1561\n        Ruby       0.99      1.00      0.99       330\n        JSON       1.00      0.96      0.98       564\n        Java       0.96      0.99      0.98       162\n        Rust       0.99      0.99      0.99      1443\n          C#       1.00      1.00      1.00      1016\n    Markdown       0.91      0.94      0.92       224\n       Julia       0.96      1.00      0.98        81\n  Javascript       0.98      0.95      0.97       386\n         PHP       0.98      1.00      0.99       141\n        YAML       0.69      0.97      0.81        37\n      Kotlin       0.99      0.99      0.99       552\n           C       0.63      0.93      0.75        59\n        Text       0.96      0.92      0.94       211\n      Elixir       0.96      0.98      0.97        44\n        Diff       1.00      1.00      1.00        77\n         GAS       0.98      1.00      0.99        45\n         C++       0.94      0.80      0.87       128\n         SQL       0.98      0.99      0.99       102\n        GLSL       0.89      0.95      0.92        62\n\n    accuracy                           0.98      8193\n   macro avg       0.94      0.97      0.95      8193\nweighted avg       0.98      0.98      0.98      8193\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# (1) Bi-LSTM","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\ntokenizer=Tokenizer(num_words= 10000,lower=True)\nnum_words = 10000\nX = df['file_content']\nY = df['language']\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X) \nX = pad_sequences(X,maxlen=100,padding='post') \ny=pd.get_dummies(Y)\nX_train, X_valid_test, y_train, y_valid_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\nX_valid, X_test, y_valid, y_test = train_test_split(X_valid_test, y_valid_test, test_size = 0.66, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:35:51.092123Z","iopub.execute_input":"2024-06-17T17:35:51.092645Z","iopub.status.idle":"2024-06-17T17:37:38.057442Z","shell.execute_reply.started":"2024-06-17T17:35:51.092606Z","shell.execute_reply":"2024-06-17T17:37:38.056777Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import tensorflow\nimport numpy as np\nfrom sklearn import metrics\nfrom tensorflow.keras.models import Sequential \nfrom keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nfrom tensorflow.keras.layers import Dropout\n\ny_encoded = np.argmax(y.values, axis=1)\nmapping = {}\nfor label, code in zip(Y, y_encoded):\n    mapping[label] = code\nsorted_keys = sorted(mapping, key=lambda x: mapping[x])\n\n\ndef traing(model):\n    es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)\n    mc = ModelCheckpoint('./'+str(model)+'-model.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)\n    history_embedding = model.fit(X_train, y_train, \n                                    epochs = 5, batch_size = 64, \n                                    validation_data=(X_valid, y_valid),\n                                    verbose = 1, callbacks= [es, mc]  )\n    return model\n\ndef testing(model):\n    y_pred =  np.argmax(model.predict(X_test), axis  =  1)\n    y_true = np.argmax(y_test.values, axis = 1)\n    print(metrics.classification_report(y_pred, y_true, target_names=sorted_keys))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:38:09.020880Z","iopub.execute_input":"2024-06-17T17:38:09.021497Z","iopub.status.idle":"2024-06-17T17:38:09.587565Z","shell.execute_reply.started":"2024-06-17T17:38:09.021441Z","shell.execute_reply":"2024-06-17T17:38:09.586709Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def get_BiLSTM():\n    EMBEDDING_DIM = 100\n    model = Sequential()\n    model.add(Embedding(input_dim = num_words,\n     output_dim = EMBEDDING_DIM,\n     input_length= X.shape[1]))\n    model.add(Bidirectional(CuDNNLSTM(100,return_sequences=True)))\n    model.add(Dropout(0.2))\n    model.add(Bidirectional(CuDNNLSTM(200,return_sequences=True)))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(100,activation = 'relu'))\n    model.add(Dense(21, activation = 'softmax'))\n    model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd',metrics = 'accuracy')\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:38:14.380084Z","iopub.execute_input":"2024-06-17T17:38:14.380624Z","iopub.status.idle":"2024-06-17T17:38:14.387854Z","shell.execute_reply.started":"2024-06-17T17:38:14.380583Z","shell.execute_reply":"2024-06-17T17:38:14.387104Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"lstm_model = traing(get_BiLSTM())","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:38:17.295038Z","iopub.execute_input":"2024-06-17T17:38:17.295564Z","iopub.status.idle":"2024-06-17T17:39:37.838519Z","shell.execute_reply.started":"2024-06-17T17:38:17.295524Z","shell.execute_reply":"2024-06-17T17:39:37.837641Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/5\n446/446 [==============================] - 19s 35ms/step - loss: 2.5110 - accuracy: 0.2394 - val_loss: 2.4199 - val_accuracy: 0.3082\n\nEpoch 00001: val_accuracy improved from -inf to 0.30815, saving model to ./<keras.engine.sequential.Sequential object at 0x7892397cc650>-model.h5\nEpoch 2/5\n446/446 [==============================] - 15s 33ms/step - loss: 2.3061 - accuracy: 0.3154 - val_loss: 2.2198 - val_accuracy: 0.3656\n\nEpoch 00002: val_accuracy improved from 0.30815 to 0.36565, saving model to ./<keras.engine.sequential.Sequential object at 0x7892397cc650>-model.h5\nEpoch 3/5\n446/446 [==============================] - 15s 33ms/step - loss: 2.0287 - accuracy: 0.4183 - val_loss: 1.8037 - val_accuracy: 0.4893\n\nEpoch 00003: val_accuracy improved from 0.36565 to 0.48930, saving model to ./<keras.engine.sequential.Sequential object at 0x7892397cc650>-model.h5\nEpoch 4/5\n446/446 [==============================] - 15s 33ms/step - loss: 1.7549 - accuracy: 0.4960 - val_loss: 1.6407 - val_accuracy: 0.5189\n\nEpoch 00004: val_accuracy improved from 0.48930 to 0.51888, saving model to ./<keras.engine.sequential.Sequential object at 0x7892397cc650>-model.h5\nEpoch 5/5\n446/446 [==============================] - 15s 34ms/step - loss: 1.5049 - accuracy: 0.5673 - val_loss: 1.3158 - val_accuracy: 0.6469\n\nEpoch 00005: val_accuracy improved from 0.51888 to 0.64686, saving model to ./<keras.engine.sequential.Sequential object at 0x7892397cc650>-model.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"testing(lstm_model)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:39:40.421819Z","iopub.execute_input":"2024-06-17T17:39:40.422115Z","iopub.status.idle":"2024-06-17T17:39:43.244623Z","shell.execute_reply.started":"2024-06-17T17:39:40.422082Z","shell.execute_reply":"2024-06-17T17:39:43.243801Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           C       0.00      0.00      0.00         0\n          C#       0.95      0.87      0.91      1095\n         C++       0.00      0.00      0.00         0\n        Dart       0.86      0.87      0.86      1510\n        Diff       0.00      0.00      0.00         0\n      Elixir       0.00      0.00      0.00         0\n         GAS       0.00      0.00      0.00         0\n        GLSL       0.00      0.00      0.00         0\n          Go       0.70      0.69      0.69       950\n        JSON       0.93      0.64      0.76       765\n        Java       0.00      0.00      0.00        14\n  Javascript       0.28      0.34      0.30       298\n       Julia       0.00      0.00      0.00         0\n      Kotlin       0.80      0.33      0.47      1315\n    Markdown       0.00      0.00      0.00         1\n         PHP       0.00      0.00      0.00         0\n        Ruby       0.44      0.39      0.41       372\n        Rust       0.91      0.74      0.81      1751\n         SQL       0.00      0.00      0.00         0\n        Text       0.00      0.00      0.00         0\n        YAML       0.00      0.00      0.00         0\n\n    accuracy                           0.67      8071\n   macro avg       0.28      0.23      0.25      8071\nweighted avg       0.82      0.67      0.72      8071\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# (2) CNN","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input,Conv1D,MaxPooling1D,Dense,GlobalMaxPooling1D,Embedding\nfrom tensorflow.keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Flatten\nfrom keras.preprocessing import sequence\n\ndef get_CNN():\n    EMBEDDING_DIM = 100\n    model = Sequential()\n    model.add(Embedding(input_dim = num_words,\n     output_dim = EMBEDDING_DIM,\n     input_length= X.shape[1]))\n    model.add(Conv1D(128, 3, activation = 'relu'))\n    model.add(MaxPooling1D(3))\n    model.add(Conv1D(64,3,activation = 'relu'))\n    model.add(Flatten())\n    model.add(Dense(250,activation = 'relu'))\n    model.add(Dense(21, activation = 'softmax'))\n    model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd',metrics = 'accuracy')\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:39:52.081517Z","iopub.execute_input":"2024-06-17T17:39:52.082538Z","iopub.status.idle":"2024-06-17T17:39:52.091136Z","shell.execute_reply.started":"2024-06-17T17:39:52.082456Z","shell.execute_reply":"2024-06-17T17:39:52.090326Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"cnn_model = traing(get_CNN())","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:39:54.971758Z","iopub.execute_input":"2024-06-17T17:39:54.972048Z","iopub.status.idle":"2024-06-17T17:40:08.393803Z","shell.execute_reply.started":"2024-06-17T17:39:54.972015Z","shell.execute_reply":"2024-06-17T17:40:08.393023Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch 1/5\n446/446 [==============================] - 7s 4ms/step - loss: 2.5726 - accuracy: 0.2412 - val_loss: 2.4574 - val_accuracy: 0.2605\n\nEpoch 00001: val_accuracy improved from -inf to 0.26052, saving model to ./<keras.engine.sequential.Sequential object at 0x7890f9a23490>-model.h5\nEpoch 2/5\n446/446 [==============================] - 2s 4ms/step - loss: 2.3911 - accuracy: 0.2698 - val_loss: 2.3766 - val_accuracy: 0.2699\n\nEpoch 00002: val_accuracy improved from 0.26052 to 0.26991, saving model to ./<keras.engine.sequential.Sequential object at 0x7890f9a23490>-model.h5\nEpoch 3/5\n446/446 [==============================] - 2s 3ms/step - loss: 2.2817 - accuracy: 0.3344 - val_loss: 2.2074 - val_accuracy: 0.3830\n\nEpoch 00003: val_accuracy improved from 0.26991 to 0.38297, saving model to ./<keras.engine.sequential.Sequential object at 0x7890f9a23490>-model.h5\nEpoch 4/5\n446/446 [==============================] - 2s 3ms/step - loss: 2.0153 - accuracy: 0.4656 - val_loss: 1.8846 - val_accuracy: 0.4927\n\nEpoch 00004: val_accuracy improved from 0.38297 to 0.49266, saving model to ./<keras.engine.sequential.Sequential object at 0x7890f9a23490>-model.h5\nEpoch 5/5\n446/446 [==============================] - 2s 3ms/step - loss: 1.7316 - accuracy: 0.5196 - val_loss: 1.6222 - val_accuracy: 0.5278\n\nEpoch 00005: val_accuracy improved from 0.49266 to 0.52778, saving model to ./<keras.engine.sequential.Sequential object at 0x7890f9a23490>-model.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"testing(cnn_model)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:40:11.197888Z","iopub.execute_input":"2024-06-17T17:40:11.198180Z","iopub.status.idle":"2024-06-17T17:40:11.649078Z","shell.execute_reply.started":"2024-06-17T17:40:11.198147Z","shell.execute_reply":"2024-06-17T17:40:11.648174Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           C       0.00      0.00      0.00         0\n          C#       0.91      0.92      0.91      1001\n         C++       0.00      0.00      0.00         0\n        Dart       0.89      0.51      0.65      2653\n        Diff       0.00      0.00      0.00         0\n      Elixir       0.00      0.00      0.00         0\n         GAS       0.00      0.00      0.00         0\n        GLSL       0.00      0.00      0.00         0\n          Go       0.52      0.47      0.49      1044\n        JSON       0.43      0.92      0.59       250\n        Java       0.00      0.00      0.00         0\n  Javascript       0.05      0.59      0.10        34\n       Julia       0.00      0.00      0.00         0\n      Kotlin       0.03      0.35      0.06        49\n    Markdown       0.00      0.00      0.00         0\n         PHP       0.00      0.00      0.00         0\n        Ruby       0.00      0.00      0.00         0\n        Rust       0.96      0.45      0.61      3040\n         SQL       0.00      0.00      0.00         0\n        Text       0.00      0.00      0.00         0\n        YAML       0.00      0.00      0.00         0\n\n    accuracy                           0.55      8071\n   macro avg       0.18      0.20      0.16      8071\nweighted avg       0.85      0.55      0.64      8071\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# (3) LSTM-CNN","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:15:24.594612Z","iopub.execute_input":"2024-06-17T07:15:24.595368Z","iopub.status.idle":"2024-06-17T07:15:24.598923Z","shell.execute_reply.started":"2024-06-17T07:15:24.595326Z","shell.execute_reply":"2024-06-17T07:15:24.598132Z"}}},{"cell_type":"code","source":"def get_LSTM_CNN():\n    EMBEDDING_DIM = 100\n    model = Sequential()\n    model.add(Embedding(input_dim = num_words,\n     output_dim = EMBEDDING_DIM,\n     input_length= X.shape[1]))\n    model.add(Dropout(0.2))\n    model.add(Bidirectional(CuDNNLSTM(100,return_sequences=True)))\n    model.add(Dropout(0.2))\n    model.add(Bidirectional(CuDNNLSTM(200,return_sequences=True)))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(128, 3, activation = 'relu'))\n    model.add(MaxPooling1D(3))\n    model.add(Conv1D(64,3,activation = 'relu'))\n    model.add(Flatten())\n    model.add(Dense(250,activation = 'relu'))\n    model.add(Dense(21, activation = 'softmax'))\n    model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd',metrics = 'accuracy')\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:40:15.612996Z","iopub.execute_input":"2024-06-17T17:40:15.613663Z","iopub.status.idle":"2024-06-17T17:40:15.622199Z","shell.execute_reply.started":"2024-06-17T17:40:15.613628Z","shell.execute_reply":"2024-06-17T17:40:15.621375Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"lstm_cnn_model = traing(get_LSTM_CNN())","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:40:20.044155Z","iopub.execute_input":"2024-06-17T17:40:20.044442Z","iopub.status.idle":"2024-06-17T17:41:42.009505Z","shell.execute_reply.started":"2024-06-17T17:40:20.044409Z","shell.execute_reply":"2024-06-17T17:41:42.008728Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1/5\n446/446 [==============================] - 18s 37ms/step - loss: 2.5427 - accuracy: 0.2209 - val_loss: 2.4775 - val_accuracy: 0.2487\n\nEpoch 00001: val_accuracy improved from -inf to 0.24874, saving model to ./<keras.engine.sequential.Sequential object at 0x7890f8f32d10>-model.h5\nEpoch 2/5\n446/446 [==============================] - 16s 35ms/step - loss: 2.4159 - accuracy: 0.2588 - val_loss: 2.4159 - val_accuracy: 0.2572\n\nEpoch 00002: val_accuracy improved from 0.24874 to 0.25716, saving model to ./<keras.engine.sequential.Sequential object at 0x7890f8f32d10>-model.h5\nEpoch 3/5\n446/446 [==============================] - 16s 35ms/step - loss: 2.2959 - accuracy: 0.3043 - val_loss: 2.1066 - val_accuracy: 0.3996\n\nEpoch 00003: val_accuracy improved from 0.25716 to 0.39957, saving model to ./<keras.engine.sequential.Sequential object at 0x7890f8f32d10>-model.h5\nEpoch 4/5\n446/446 [==============================] - 16s 35ms/step - loss: 2.0459 - accuracy: 0.4087 - val_loss: 2.0147 - val_accuracy: 0.4102\n\nEpoch 00004: val_accuracy improved from 0.39957 to 0.41015, saving model to ./<keras.engine.sequential.Sequential object at 0x7890f8f32d10>-model.h5\nEpoch 5/5\n446/446 [==============================] - 16s 35ms/step - loss: 1.8260 - accuracy: 0.4757 - val_loss: 1.7084 - val_accuracy: 0.5227\n\nEpoch 00005: val_accuracy improved from 0.41015 to 0.52273, saving model to ./<keras.engine.sequential.Sequential object at 0x7890f8f32d10>-model.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"testing(lstm_cnn_model)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:41:43.871653Z","iopub.execute_input":"2024-06-17T17:41:43.872391Z","iopub.status.idle":"2024-06-17T17:41:46.698795Z","shell.execute_reply.started":"2024-06-17T17:41:43.872345Z","shell.execute_reply":"2024-06-17T17:41:46.698006Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           C       0.00      0.00      0.00         0\n          C#       0.90      0.90      0.90      1005\n         C++       0.00      0.00      0.00         0\n        Dart       0.89      0.49      0.63      2766\n        Diff       0.00      0.00      0.00         0\n      Elixir       0.00      0.00      0.00         0\n         GAS       0.00      0.00      0.00         0\n        GLSL       0.00      0.00      0.00         0\n          Go       0.62      0.37      0.47      1581\n        JSON       0.46      0.77      0.58       316\n        Java       0.00      0.00      0.00         3\n  Javascript       0.03      0.17      0.05        65\n       Julia       0.00      0.00      0.00         0\n      Kotlin       0.00      0.00      0.00         0\n    Markdown       0.00      0.00      0.00         0\n         PHP       0.00      0.00      0.00         0\n        Ruby       0.00      0.00      0.00         0\n        Rust       0.87      0.53      0.66      2335\n         SQL       0.00      0.00      0.00         0\n        Text       0.00      0.00      0.00         0\n        YAML       0.00      0.00      0.00         0\n\n    accuracy                           0.54      8071\n   macro avg       0.18      0.15      0.16      8071\nweighted avg       0.81      0.54      0.63      8071\n\n","output_type":"stream"}]}]}